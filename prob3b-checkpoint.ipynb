{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af78a16c-1f14-41bc-97d7-65fbf0007a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c297901-e256-4d6f-8406-19273e00f953",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57664f00-37c6-4e8f-8533-111dd729f936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Transform to resize CIFAR-10 to 224x224 as AlexNet expects\n",
    "transform = transforms.Compose([transforms.Resize(224),transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591a3288-b183-4e1e-991e-c943b2c218b0",
   "metadata": {},
   "source": [
    "# Define AlexNet (Modified for CIFAR-10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adaa7361-63a1-4be9-9314-b7e25924cef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), 256 * 6 * 6)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efea92b-f985-47c8-a8ec-202c103ac31c",
   "metadata": {},
   "source": [
    "# Initialize model, loss and optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffe0cc36-564b-4343-adde-561194d9e9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AlexNet(num_classes=10).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f960196e-b73e-41e4-8189-6f9bd07c6f3a",
   "metadata": {},
   "source": [
    "# Training loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a195f20-f5b5-470f-8636-974ec88f8a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1/782\n",
      "Loss: 2.1300272941589355\n",
      "Batch 2/782\n",
      "Loss: 2.1846673488616943\n",
      "Batch 3/782\n",
      "Loss: 2.1770687103271484\n",
      "Batch 4/782\n",
      "Loss: 2.1449365615844727\n",
      "Batch 5/782\n",
      "Loss: 2.1892776489257812\n",
      "Batch 6/782\n",
      "Loss: 2.0277035236358643\n",
      "Batch 7/782\n",
      "Loss: 1.9602346420288086\n",
      "Batch 8/782\n",
      "Loss: 2.333920478820801\n",
      "Batch 9/782\n",
      "Loss: 2.1307995319366455\n",
      "Batch 10/782\n",
      "Loss: 1.9515713453292847\n",
      "Batch 11/782\n",
      "Loss: 2.1419339179992676\n",
      "Batch 12/782\n",
      "Loss: 2.1169381141662598\n",
      "Batch 13/782\n",
      "Loss: 2.1794862747192383\n",
      "Batch 14/782\n",
      "Loss: 2.0674972534179688\n",
      "Batch 15/782\n",
      "Loss: 2.131045341491699\n",
      "Batch 16/782\n",
      "Loss: 2.0797414779663086\n",
      "Batch 17/782\n",
      "Loss: 2.1580255031585693\n",
      "Batch 18/782\n",
      "Loss: 2.1034293174743652\n",
      "Batch 19/782\n",
      "Loss: 1.9906948804855347\n",
      "Batch 20/782\n",
      "Loss: 2.0310258865356445\n",
      "Batch 21/782\n",
      "Loss: 2.0720396041870117\n",
      "Batch 22/782\n",
      "Loss: 2.1487958431243896\n",
      "Batch 23/782\n",
      "Loss: 2.097804069519043\n",
      "Batch 24/782\n",
      "Loss: 2.1603264808654785\n",
      "Batch 25/782\n",
      "Loss: 2.014016628265381\n",
      "Batch 26/782\n",
      "Loss: 1.9603497982025146\n",
      "Batch 27/782\n",
      "Loss: 1.8889824151992798\n",
      "Batch 28/782\n",
      "Loss: 2.0198018550872803\n",
      "Batch 29/782\n",
      "Loss: 1.9861631393432617\n",
      "Batch 30/782\n",
      "Loss: 2.257172107696533\n",
      "Batch 31/782\n",
      "Loss: 2.2578418254852295\n",
      "Batch 32/782\n",
      "Loss: 2.2316601276397705\n",
      "Batch 33/782\n",
      "Loss: 2.1565680503845215\n",
      "Batch 34/782\n",
      "Loss: 2.016629695892334\n",
      "Batch 35/782\n",
      "Loss: 2.153672933578491\n",
      "Batch 36/782\n",
      "Loss: 2.0320937633514404\n",
      "Batch 37/782\n",
      "Loss: 2.111685037612915\n",
      "Batch 38/782\n",
      "Loss: 2.039823293685913\n",
      "Batch 39/782\n",
      "Loss: 2.143036365509033\n",
      "Batch 40/782\n",
      "Loss: 2.162599802017212\n",
      "Batch 41/782\n",
      "Loss: 1.947464108467102\n",
      "Batch 42/782\n",
      "Loss: 2.0204813480377197\n",
      "Batch 43/782\n",
      "Loss: 1.9814451932907104\n",
      "Batch 44/782\n",
      "Loss: 2.081496000289917\n",
      "Batch 45/782\n",
      "Loss: 2.162416458129883\n",
      "Batch 46/782\n",
      "Loss: 1.9916232824325562\n",
      "Batch 47/782\n",
      "Loss: 2.0354154109954834\n",
      "Batch 48/782\n",
      "Loss: 2.0124120712280273\n",
      "Batch 49/782\n",
      "Loss: 2.0462615489959717\n",
      "Batch 50/782\n",
      "Loss: 1.9939916133880615\n",
      "Batch 51/782\n",
      "Loss: 1.9735873937606812\n",
      "Batch 52/782\n",
      "Loss: 2.043524980545044\n",
      "Batch 53/782\n",
      "Loss: 1.9729269742965698\n",
      "Batch 54/782\n",
      "Loss: 1.986560583114624\n",
      "Batch 55/782\n",
      "Loss: 1.9235930442810059\n",
      "Batch 56/782\n",
      "Loss: 2.1315107345581055\n",
      "Batch 57/782\n",
      "Loss: 2.0216128826141357\n",
      "Batch 58/782\n",
      "Loss: 2.072831869125366\n",
      "Batch 59/782\n",
      "Loss: 2.0823657512664795\n",
      "Batch 60/782\n",
      "Loss: 1.9826703071594238\n",
      "Batch 61/782\n",
      "Loss: 2.070199966430664\n",
      "Batch 62/782\n",
      "Loss: 2.1291422843933105\n",
      "Batch 63/782\n",
      "Loss: 2.002436399459839\n",
      "Batch 64/782\n",
      "Loss: 2.0148868560791016\n",
      "Batch 65/782\n",
      "Loss: 2.0656278133392334\n",
      "Batch 66/782\n",
      "Loss: 1.9334032535552979\n",
      "Batch 67/782\n",
      "Loss: 2.0014615058898926\n",
      "Batch 68/782\n",
      "Loss: 1.9692195653915405\n",
      "Batch 69/782\n",
      "Loss: 1.8254673480987549\n",
      "Batch 70/782\n",
      "Loss: 2.052574872970581\n",
      "Batch 71/782\n",
      "Loss: 1.800061583518982\n",
      "Batch 72/782\n",
      "Loss: 1.9958176612854004\n",
      "Batch 73/782\n",
      "Loss: 2.2326693534851074\n",
      "Batch 74/782\n",
      "Loss: 1.9194334745407104\n",
      "Batch 75/782\n",
      "Loss: 1.8336732387542725\n",
      "Batch 76/782\n",
      "Loss: 1.9548112154006958\n",
      "Batch 77/782\n",
      "Loss: 1.9538394212722778\n",
      "Batch 78/782\n",
      "Loss: 2.072453498840332\n",
      "Batch 79/782\n",
      "Loss: 2.0503833293914795\n",
      "Batch 80/782\n",
      "Loss: 2.0144922733306885\n",
      "Batch 81/782\n",
      "Loss: 2.2219655513763428\n",
      "Batch 82/782\n",
      "Loss: 1.9840184450149536\n",
      "Batch 83/782\n",
      "Loss: 2.092491865158081\n",
      "Batch 84/782\n",
      "Loss: 1.9615522623062134\n",
      "Batch 85/782\n",
      "Loss: 2.006216049194336\n",
      "Batch 86/782\n",
      "Loss: 2.025771379470825\n",
      "Batch 87/782\n",
      "Loss: 2.010622024536133\n",
      "Batch 88/782\n",
      "Loss: 1.8950132131576538\n",
      "Batch 89/782\n",
      "Loss: 2.0358059406280518\n",
      "Batch 90/782\n",
      "Loss: 2.1212170124053955\n",
      "Batch 91/782\n",
      "Loss: 1.9043675661087036\n",
      "Batch 92/782\n",
      "Loss: 2.0184082984924316\n",
      "Batch 93/782\n",
      "Loss: 1.7877850532531738\n",
      "Batch 94/782\n",
      "Loss: 1.9040659666061401\n",
      "Batch 95/782\n",
      "Loss: 1.7553513050079346\n",
      "Batch 96/782\n",
      "Loss: 2.0743048191070557\n",
      "Batch 97/782\n",
      "Loss: 1.7997983694076538\n",
      "Batch 98/782\n",
      "Loss: 1.920535922050476\n",
      "Batch 99/782\n",
      "Loss: 1.9525524377822876\n",
      "Batch 100/782\n",
      "Loss: 1.8653732538223267\n",
      "Batch 101/782\n",
      "Loss: 1.8377481698989868\n",
      "Batch 102/782\n",
      "Loss: 1.8696843385696411\n",
      "Batch 103/782\n",
      "Loss: 1.814162015914917\n",
      "Batch 104/782\n",
      "Loss: 1.9292110204696655\n",
      "Batch 105/782\n",
      "Loss: 1.8748362064361572\n",
      "Batch 106/782\n",
      "Loss: 2.036194324493408\n",
      "Batch 107/782\n",
      "Loss: 1.9803249835968018\n",
      "Batch 108/782\n",
      "Loss: 1.9773588180541992\n",
      "Batch 109/782\n",
      "Loss: 1.9269671440124512\n",
      "Batch 110/782\n",
      "Loss: 1.8238111734390259\n",
      "Batch 111/782\n",
      "Loss: 1.8960063457489014\n",
      "Batch 112/782\n",
      "Loss: 1.997304081916809\n",
      "Batch 113/782\n",
      "Loss: 1.9715226888656616\n",
      "Batch 114/782\n",
      "Loss: 2.0773305892944336\n",
      "Batch 115/782\n",
      "Loss: 1.8233072757720947\n",
      "Batch 116/782\n",
      "Loss: 1.8513493537902832\n",
      "Batch 117/782\n",
      "Loss: 1.9858036041259766\n",
      "Batch 118/782\n",
      "Loss: 1.9146713018417358\n",
      "Batch 119/782\n",
      "Loss: 1.9659664630889893\n",
      "Batch 120/782\n",
      "Loss: 1.8070828914642334\n",
      "Batch 121/782\n",
      "Loss: 1.9521652460098267\n",
      "Batch 122/782\n",
      "Loss: 1.9197883605957031\n",
      "Batch 123/782\n",
      "Loss: 1.7824323177337646\n",
      "Batch 124/782\n",
      "Loss: 1.9486790895462036\n",
      "Batch 125/782\n",
      "Loss: 2.033442735671997\n",
      "Batch 126/782\n",
      "Loss: 1.8314765691757202\n",
      "Batch 127/782\n",
      "Loss: 2.0644845962524414\n",
      "Batch 128/782\n",
      "Loss: 1.9741877317428589\n",
      "Batch 129/782\n",
      "Loss: 1.9647966623306274\n",
      "Batch 130/782\n",
      "Loss: 1.974614143371582\n",
      "Batch 131/782\n",
      "Loss: 1.8313342332839966\n",
      "Batch 132/782\n",
      "Loss: 1.8499621152877808\n",
      "Batch 133/782\n",
      "Loss: 1.9736764430999756\n",
      "Batch 134/782\n",
      "Loss: 1.8573025465011597\n",
      "Batch 135/782\n",
      "Loss: 1.817204475402832\n",
      "Batch 136/782\n",
      "Loss: 1.749916911125183\n",
      "Batch 137/782\n",
      "Loss: 1.8957874774932861\n",
      "Batch 138/782\n",
      "Loss: 2.0476691722869873\n",
      "Batch 139/782\n",
      "Loss: 1.7994537353515625\n",
      "Batch 140/782\n",
      "Loss: 1.8652511835098267\n",
      "Batch 141/782\n",
      "Loss: 2.1388330459594727\n",
      "Batch 142/782\n",
      "Loss: 1.897939682006836\n",
      "Batch 143/782\n",
      "Loss: 1.8124010562896729\n",
      "Batch 144/782\n",
      "Loss: 2.1096339225769043\n",
      "Batch 145/782\n",
      "Loss: 1.8507863283157349\n",
      "Batch 146/782\n",
      "Loss: 1.8971682786941528\n",
      "Batch 147/782\n",
      "Loss: 1.8885884284973145\n",
      "Batch 148/782\n",
      "Loss: 1.8061151504516602\n",
      "Batch 149/782\n",
      "Loss: 2.054821252822876\n",
      "Batch 150/782\n",
      "Loss: 1.8689216375350952\n",
      "Batch 151/782\n",
      "Loss: 1.855579137802124\n",
      "Batch 152/782\n",
      "Loss: 1.8960456848144531\n",
      "Batch 153/782\n",
      "Loss: 1.6095025539398193\n",
      "Batch 154/782\n",
      "Loss: 2.0087382793426514\n",
      "Batch 155/782\n",
      "Loss: 1.9457710981369019\n",
      "Batch 156/782\n",
      "Loss: 1.6883103847503662\n",
      "Batch 157/782\n",
      "Loss: 1.8976311683654785\n",
      "Batch 158/782\n",
      "Loss: 1.7471237182617188\n",
      "Batch 159/782\n",
      "Loss: 2.0510454177856445\n",
      "Batch 160/782\n",
      "Loss: 1.857996940612793\n",
      "Batch 161/782\n",
      "Loss: 1.810185194015503\n",
      "Batch 162/782\n",
      "Loss: 1.6502398252487183\n",
      "Batch 163/782\n",
      "Loss: 1.7636548280715942\n",
      "Batch 164/782\n",
      "Loss: 1.8574110269546509\n",
      "Batch 165/782\n",
      "Loss: 1.9133872985839844\n",
      "Batch 166/782\n",
      "Loss: 1.8091509342193604\n",
      "Batch 167/782\n",
      "Loss: 1.7282192707061768\n",
      "Batch 168/782\n",
      "Loss: 1.9406769275665283\n",
      "Batch 169/782\n",
      "Loss: 1.8192288875579834\n",
      "Batch 170/782\n",
      "Loss: 2.000131845474243\n",
      "Batch 171/782\n",
      "Loss: 1.7846671342849731\n",
      "Batch 172/782\n",
      "Loss: 1.7430428266525269\n",
      "Batch 173/782\n",
      "Loss: 1.7575379610061646\n",
      "Batch 174/782\n",
      "Loss: 1.6934775114059448\n",
      "Batch 175/782\n",
      "Loss: 1.8037809133529663\n",
      "Batch 176/782\n",
      "Loss: 1.9705890417099\n",
      "Batch 177/782\n",
      "Loss: 1.7437797784805298\n",
      "Batch 178/782\n",
      "Loss: 1.9395488500595093\n",
      "Batch 179/782\n",
      "Loss: 2.002497911453247\n",
      "Batch 180/782\n",
      "Loss: 2.0972394943237305\n",
      "Batch 181/782\n",
      "Loss: 1.8669332265853882\n",
      "Batch 182/782\n",
      "Loss: 1.7903934717178345\n",
      "Batch 183/782\n",
      "Loss: 1.9360345602035522\n",
      "Batch 184/782\n",
      "Loss: 1.9132267236709595\n",
      "Batch 185/782\n",
      "Loss: 1.8724039793014526\n",
      "Batch 186/782\n",
      "Loss: 2.0363450050354004\n",
      "Batch 187/782\n",
      "Loss: 1.8930134773254395\n",
      "Batch 188/782\n",
      "Loss: 1.922141194343567\n",
      "Batch 189/782\n",
      "Loss: 2.020549774169922\n",
      "Batch 190/782\n",
      "Loss: 1.8290815353393555\n",
      "Batch 191/782\n",
      "Loss: 1.9805082082748413\n",
      "Batch 192/782\n",
      "Loss: 2.1120591163635254\n",
      "Batch 193/782\n",
      "Loss: 1.8033398389816284\n",
      "Batch 194/782\n",
      "Loss: 2.0440306663513184\n",
      "Batch 195/782\n",
      "Loss: 1.9805516004562378\n",
      "Batch 196/782\n",
      "Loss: 1.725155234336853\n",
      "Batch 197/782\n",
      "Loss: 1.7567232847213745\n",
      "Batch 198/782\n",
      "Loss: 1.9595426321029663\n",
      "Batch 199/782\n",
      "Loss: 1.9804905652999878\n",
      "Batch 200/782\n",
      "Loss: 2.0410966873168945\n",
      "Batch 201/782\n",
      "Loss: 2.0041708946228027\n",
      "Batch 202/782\n",
      "Loss: 1.967748999595642\n",
      "Batch 203/782\n",
      "Loss: 2.038475275039673\n",
      "Batch 204/782\n",
      "Loss: 1.9294356107711792\n",
      "Batch 205/782\n",
      "Loss: 1.9440982341766357\n",
      "Batch 206/782\n",
      "Loss: 1.9112350940704346\n",
      "Batch 207/782\n",
      "Loss: 1.8115960359573364\n",
      "Batch 208/782\n",
      "Loss: 1.828065276145935\n",
      "Batch 209/782\n",
      "Loss: 1.8260128498077393\n",
      "Batch 210/782\n",
      "Loss: 1.8251291513442993\n",
      "Batch 211/782\n",
      "Loss: 1.7314305305480957\n",
      "Batch 212/782\n",
      "Loss: 1.8228274583816528\n",
      "Batch 213/782\n",
      "Loss: 1.8824533224105835\n",
      "Batch 214/782\n",
      "Loss: 1.8944414854049683\n",
      "Batch 215/782\n",
      "Loss: 1.811786413192749\n",
      "Batch 216/782\n",
      "Loss: 1.9091659784317017\n",
      "Batch 217/782\n",
      "Loss: 1.908482313156128\n",
      "Batch 218/782\n",
      "Loss: 1.778085470199585\n",
      "Batch 219/782\n",
      "Loss: 1.7652822732925415\n",
      "Batch 220/782\n",
      "Loss: 1.800004482269287\n",
      "Batch 221/782\n",
      "Loss: 1.806759238243103\n",
      "Batch 222/782\n",
      "Loss: 1.821249008178711\n",
      "Batch 223/782\n",
      "Loss: 1.7517871856689453\n",
      "Batch 224/782\n",
      "Loss: 1.655786395072937\n",
      "Batch 225/782\n",
      "Loss: 1.803922176361084\n",
      "Batch 226/782\n",
      "Loss: 1.5314486026763916\n",
      "Batch 227/782\n",
      "Loss: 1.7156429290771484\n",
      "Batch 228/782\n",
      "Loss: 1.7437793016433716\n",
      "Batch 229/782\n",
      "Loss: 1.9162123203277588\n",
      "Batch 230/782\n",
      "Loss: 1.9024971723556519\n",
      "Batch 231/782\n",
      "Loss: 1.808709979057312\n",
      "Batch 232/782\n",
      "Loss: 2.0787808895111084\n",
      "Batch 233/782\n",
      "Loss: 1.8527512550354004\n",
      "Batch 234/782\n",
      "Loss: 1.9093217849731445\n",
      "Batch 235/782\n",
      "Loss: 1.8349366188049316\n",
      "Batch 236/782\n",
      "Loss: 1.8345043659210205\n",
      "Batch 237/782\n",
      "Loss: 1.6916760206222534\n",
      "Batch 238/782\n",
      "Loss: 1.8478341102600098\n",
      "Batch 239/782\n",
      "Loss: 1.9071518182754517\n",
      "Batch 240/782\n",
      "Loss: 2.02382230758667\n",
      "Batch 241/782\n",
      "Loss: 1.815470576286316\n",
      "Batch 242/782\n",
      "Loss: 1.7123591899871826\n",
      "Batch 243/782\n",
      "Loss: 1.9702322483062744\n",
      "Batch 244/782\n",
      "Loss: 1.8887697458267212\n",
      "Batch 245/782\n",
      "Loss: 1.7283269166946411\n",
      "Batch 246/782\n",
      "Loss: 1.8866164684295654\n",
      "Batch 247/782\n",
      "Loss: 1.8608880043029785\n",
      "Batch 248/782\n",
      "Loss: 1.8870322704315186\n",
      "Batch 249/782\n",
      "Loss: 1.7086856365203857\n",
      "Batch 250/782\n",
      "Loss: 1.587436556816101\n",
      "Batch 251/782\n",
      "Loss: 1.8667868375778198\n",
      "Batch 252/782\n",
      "Loss: 1.9248161315917969\n",
      "Batch 253/782\n",
      "Loss: 1.7490391731262207\n",
      "Batch 254/782\n",
      "Loss: 1.7200422286987305\n",
      "Batch 255/782\n",
      "Loss: 1.6450060606002808\n",
      "Batch 256/782\n",
      "Loss: 1.8829004764556885\n",
      "Batch 257/782\n",
      "Loss: 1.921533465385437\n",
      "Batch 258/782\n",
      "Loss: 1.9480171203613281\n",
      "Batch 259/782\n",
      "Loss: 1.7467354536056519\n",
      "Batch 260/782\n",
      "Loss: 1.8177053928375244\n",
      "Batch 261/782\n",
      "Loss: 1.9202812910079956\n",
      "Batch 262/782\n",
      "Loss: 1.7996536493301392\n",
      "Batch 263/782\n",
      "Loss: 1.8094483613967896\n",
      "Batch 264/782\n",
      "Loss: 1.731891393661499\n",
      "Batch 265/782\n",
      "Loss: 1.8445734977722168\n",
      "Batch 266/782\n",
      "Loss: 1.6565957069396973\n",
      "Batch 267/782\n",
      "Loss: 1.7035999298095703\n",
      "Batch 268/782\n",
      "Loss: 1.7150133848190308\n",
      "Batch 269/782\n",
      "Loss: 2.0288164615631104\n",
      "Batch 270/782\n",
      "Loss: 1.9143024682998657\n",
      "Batch 271/782\n",
      "Loss: 1.9286004304885864\n",
      "Batch 272/782\n",
      "Loss: 1.896103024482727\n",
      "Batch 273/782\n",
      "Loss: 1.8817780017852783\n",
      "Batch 274/782\n",
      "Loss: 1.7994688749313354\n",
      "Batch 275/782\n",
      "Loss: 1.7221333980560303\n",
      "Batch 276/782\n",
      "Loss: 1.6217488050460815\n",
      "Batch 277/782\n",
      "Loss: 1.707585334777832\n",
      "Batch 278/782\n",
      "Loss: 1.948319435119629\n",
      "Batch 279/782\n",
      "Loss: 1.8205755949020386\n",
      "Batch 280/782\n",
      "Loss: 1.9207639694213867\n",
      "Batch 281/782\n",
      "Loss: 1.9331177473068237\n",
      "Batch 282/782\n",
      "Loss: 1.781982421875\n",
      "Batch 283/782\n",
      "Loss: 1.6802598237991333\n",
      "Batch 284/782\n",
      "Loss: 1.7524994611740112\n",
      "Batch 285/782\n",
      "Loss: 1.8407940864562988\n",
      "Batch 286/782\n",
      "Loss: 2.0171926021575928\n",
      "Batch 287/782\n",
      "Loss: 1.675100326538086\n",
      "Batch 288/782\n",
      "Loss: 1.7837227582931519\n",
      "Batch 289/782\n",
      "Loss: 1.7366821765899658\n",
      "Batch 290/782\n",
      "Loss: 1.655677080154419\n",
      "Batch 291/782\n",
      "Loss: 1.7227048873901367\n",
      "Batch 292/782\n",
      "Loss: 1.9430474042892456\n",
      "Batch 293/782\n",
      "Loss: 1.5518170595169067\n",
      "Batch 294/782\n",
      "Loss: 1.7529619932174683\n",
      "Batch 295/782\n",
      "Loss: 1.636160135269165\n",
      "Batch 296/782\n",
      "Loss: 1.8036810159683228\n",
      "Batch 297/782\n",
      "Loss: 1.7856824398040771\n",
      "Batch 298/782\n",
      "Loss: 1.8773020505905151\n",
      "Batch 299/782\n",
      "Loss: 1.7149263620376587\n",
      "Batch 300/782\n",
      "Loss: 1.9057590961456299\n",
      "Batch 301/782\n",
      "Loss: 1.94895339012146\n",
      "Batch 302/782\n",
      "Loss: 1.7844774723052979\n",
      "Batch 303/782\n",
      "Loss: 1.6708102226257324\n",
      "Batch 304/782\n",
      "Loss: 1.738211989402771\n",
      "Batch 305/782\n",
      "Loss: 1.9277808666229248\n",
      "Batch 306/782\n",
      "Loss: 1.979287028312683\n",
      "Batch 307/782\n",
      "Loss: 1.6817985773086548\n",
      "Batch 308/782\n",
      "Loss: 1.7155596017837524\n",
      "Batch 309/782\n",
      "Loss: 1.5189954042434692\n",
      "Batch 310/782\n",
      "Loss: 1.935913324356079\n",
      "Batch 311/782\n",
      "Loss: 1.8588050603866577\n",
      "Batch 312/782\n",
      "Loss: 1.7989284992218018\n",
      "Batch 313/782\n",
      "Loss: 1.594118595123291\n",
      "Batch 314/782\n",
      "Loss: 1.7987053394317627\n",
      "Batch 315/782\n",
      "Loss: 1.677245020866394\n",
      "Batch 316/782\n",
      "Loss: 1.608080506324768\n",
      "Batch 317/782\n",
      "Loss: 2.036432981491089\n",
      "Batch 318/782\n",
      "Loss: 1.7705529928207397\n",
      "Batch 319/782\n",
      "Loss: 1.6925300359725952\n",
      "Batch 320/782\n",
      "Loss: 1.8933223485946655\n",
      "Batch 321/782\n",
      "Loss: 1.6978188753128052\n",
      "Batch 322/782\n",
      "Loss: 1.8457008600234985\n",
      "Batch 323/782\n",
      "Loss: 1.7114825248718262\n",
      "Batch 324/782\n",
      "Loss: 1.8591008186340332\n",
      "Batch 325/782\n",
      "Loss: 1.779544472694397\n",
      "Batch 326/782\n",
      "Loss: 1.8444900512695312\n",
      "Batch 327/782\n",
      "Loss: 1.813339114189148\n",
      "Batch 328/782\n",
      "Loss: 1.7606914043426514\n",
      "Batch 329/782\n",
      "Loss: 1.7765318155288696\n",
      "Batch 330/782\n",
      "Loss: 1.822068214416504\n",
      "Batch 331/782\n",
      "Loss: 1.646536946296692\n",
      "Batch 332/782\n",
      "Loss: 1.7401015758514404\n",
      "Batch 333/782\n",
      "Loss: 1.9549012184143066\n",
      "Batch 334/782\n",
      "Loss: 1.7530529499053955\n",
      "Batch 335/782\n",
      "Loss: 1.9642784595489502\n",
      "Batch 336/782\n",
      "Loss: 1.6292479038238525\n",
      "Batch 337/782\n",
      "Loss: 1.559674859046936\n",
      "Batch 338/782\n",
      "Loss: 1.8205482959747314\n",
      "Batch 339/782\n",
      "Loss: 1.9781426191329956\n",
      "Batch 340/782\n",
      "Loss: 1.8632452487945557\n",
      "Batch 341/782\n",
      "Loss: 1.7114814519882202\n",
      "Batch 342/782\n",
      "Loss: 1.7765398025512695\n",
      "Batch 343/782\n",
      "Loss: 1.68789541721344\n",
      "Batch 344/782\n",
      "Loss: 2.0198376178741455\n",
      "Batch 345/782\n",
      "Loss: 1.6430025100708008\n",
      "Batch 346/782\n",
      "Loss: 1.5718175172805786\n",
      "Batch 347/782\n",
      "Loss: 1.8842979669570923\n",
      "Batch 348/782\n",
      "Loss: 1.6716057062149048\n",
      "Batch 349/782\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBatch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m----> 8\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[0;32m      9\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[4], line 30\u001b[0m, in \u001b[0;36mAlexNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 30\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures(x)\n\u001b[0;32m     31\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m256\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m6\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m6\u001b[39m)\n\u001b[0;32m     32\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(x)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[0;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    548\u001b[0m     )\n\u001b[1;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[0;32m    550\u001b[0m     \u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups\n\u001b[0;32m    551\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        print(f'Batch {i+1}/{len(train_loader)}')\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        print(f'Loss: {loss.item()}')\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937a1d10-431f-4860-9e88-e2b7f5edb03d",
   "metadata": {},
   "source": [
    "# Evaluate on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0de7b88-e064-4cad-96c0-b4672d762b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of AlexNet on CIFAR-10: 37.07%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of AlexNet on CIFAR-10: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76f550b-4c46-4efd-8b62-35a9336bf3ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
